{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V28",
      "authorship_tag": "ABX9TyNf3zdUapIH+FuzRTvexGVO",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ChintPatel/CMPE255-Decision-trees-and-ensemble/blob/main/gradient_boost_ranking.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install scikit-surprise"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p1_ZHU76kiQT",
        "outputId": "cfc92370-8d43-4fb1-f616-d90342ce3fb8"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting scikit-surprise\n",
            "  Downloading scikit_surprise-1.1.4.tar.gz (154 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/154.4 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m153.6/154.4 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m154.4/154.4 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-surprise) (1.4.2)\n",
            "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.10/dist-packages (from scikit-surprise) (1.26.4)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from scikit-surprise) (1.13.1)\n",
            "Building wheels for collected packages: scikit-surprise\n",
            "  Building wheel for scikit-surprise (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for scikit-surprise: filename=scikit_surprise-1.1.4-cp310-cp310-linux_x86_64.whl size=2357288 sha256=e9e7f836551463c7da343fe7c3a4a34ccb5832a893c343e524b5a7675bd969eb\n",
            "  Stored in directory: /root/.cache/pip/wheels/4b/3f/df/6acbf0a40397d9bf3ff97f582cc22fb9ce66adde75bc71fd54\n",
            "Successfully built scikit-surprise\n",
            "Installing collected packages: scikit-surprise\n",
            "Successfully installed scikit-surprise-1.1.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install xgboost catboost lightgbm scikit-learn pandas\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oJkR3dhZl5xr",
        "outputId": "67f5d871-e3ea-4633-fc3c-03282a185d87"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting xgboost\n",
            "  Using cached xgboost-2.1.3-py3-none-manylinux_2_28_x86_64.whl.metadata (2.1 kB)\n",
            "Requirement already satisfied: catboost in /usr/local/lib/python3.10/dist-packages (1.2.7)\n",
            "Collecting lightgbm\n",
            "  Using cached lightgbm-4.5.0-py3-none-manylinux_2_28_x86_64.whl.metadata (17 kB)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.5.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from xgboost) (1.26.4)\n",
            "Collecting nvidia-nccl-cu12 (from xgboost)\n",
            "  Using cached nvidia_nccl_cu12-2.23.4-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from xgboost) (1.13.1)\n",
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.10/dist-packages (from catboost) (0.20.3)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from catboost) (3.8.0)\n",
            "Requirement already satisfied: plotly in /usr/local/lib/python3.10/dist-packages (from catboost) (5.24.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from catboost) (1.16.0)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.5.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (4.55.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (1.4.7)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (24.2)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (11.0.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (3.2.0)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from plotly->catboost) (9.0.0)\n",
            "Using cached xgboost-2.1.3-py3-none-manylinux_2_28_x86_64.whl (153.9 MB)\n",
            "Using cached lightgbm-4.5.0-py3-none-manylinux_2_28_x86_64.whl (3.6 MB)\n",
            "Using cached nvidia_nccl_cu12-2.23.4-py3-none-manylinux2014_x86_64.whl (199.0 MB)\n",
            "Installing collected packages: nvidia-nccl-cu12, xgboost, lightgbm\n",
            "Successfully installed lightgbm-4.5.0 nvidia-nccl-cu12-2.23.4 xgboost-2.1.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Import libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import ndcg_score\n",
        "import xgboost as xgb\n",
        "import catboost as cb\n",
        "import lightgbm as lgb\n",
        "\n",
        "# Load the MovieLens dataset\n",
        "# Install the dataset package if not already installed: pip install surprise\n",
        "from surprise import Dataset\n",
        "\n",
        "data = Dataset.load_builtin(\"ml-100k\")  # MovieLens 100k dataset\n",
        "raw_data = pd.DataFrame(data.raw_ratings, columns=[\"user\", \"item\", \"rating\", \"timestamp\"])\n",
        "\n",
        "# Preprocess dataset\n",
        "raw_data[\"user\"] = raw_data[\"user\"].astype(int)\n",
        "raw_data[\"item\"] = raw_data[\"item\"].astype(int)\n",
        "raw_data[\"rating\"] = raw_data[\"rating\"].astype(float)\n",
        "\n",
        "# Create features and target\n",
        "features = raw_data[[\"user\", \"item\"]]\n",
        "target = raw_data[\"rating\"]\n",
        "\n",
        "# Simulate groups as \"users\"\n",
        "groups = raw_data[\"user\"]\n",
        "\n",
        "# Split into training and testing\n",
        "X_train, X_test, y_train, y_test, groups_train, groups_test = train_test_split(\n",
        "    features, target, groups, test_size=0.2, random_state=42\n",
        ")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ImynLXBmjymQ",
        "outputId": "9bad6a3c-ae90-4cc5-f407-11a84cbed893"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset ml-100k could not be found. Do you want to download it? [Y/n] Y\n",
            "Trying to download dataset from https://files.grouplens.org/datasets/movielens/ml-100k.zip...\n",
            "Done! Dataset ml-100k has been saved to /root/.surprise_data/ml-100k\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Train data for CatBoost\n",
        "train_data = pd.concat(\n",
        "    [X_train.reset_index(drop=True), y_train.reset_index(drop=True), groups_train.reset_index(drop=True)],\n",
        "    axis=1\n",
        ")\n",
        "train_data.columns = [\"user\", \"item\", \"rating\", \"group_id\"]  # Assign distinct column names\n",
        "train_data = train_data.sort_values(by=\"group_id\")\n",
        "X_train_sorted = train_data[[\"user\", \"item\"]]\n",
        "y_train_sorted = train_data[\"rating\"]\n",
        "groups_train_sorted = train_data[\"group_id\"]\n",
        "\n",
        "# Test data for CatBoost\n",
        "test_data = pd.concat(\n",
        "    [X_test.reset_index(drop=True), y_test.reset_index(drop=True), groups_test.reset_index(drop=True)],\n",
        "    axis=1\n",
        ")\n",
        "test_data.columns = [\"user\", \"item\", \"rating\", \"group_id\"]  # Assign distinct column names\n",
        "test_data = test_data.sort_values(by=\"group_id\")\n",
        "X_test_sorted = test_data[[\"user\", \"item\"]]\n",
        "y_test_sorted = test_data[\"rating\"]\n",
        "groups_test_sorted = test_data[\"group_id\"]\n",
        "\n",
        "# Models for ranking\n",
        "models = {\n",
        "    \"XGBoost\": xgb.XGBRanker(\n",
        "        objective=\"rank:pairwise\", learning_rate=0.1, n_estimators=300, max_depth=8, random_state=42\n",
        "    ),\n",
        "    \"CatBoost\": cb.CatBoostRanker(\n",
        "        iterations=300, learning_rate=0.1, depth=8, verbose=0, random_seed=42\n",
        "    ),\n",
        "    \"LightGBM\": lgb.LGBMRanker(\n",
        "        boosting_type=\"gbdt\", objective=\"lambdarank\", learning_rate=0.1, n_estimators=300, max_depth=8, random_state=42\n",
        "    ),\n",
        "}\n",
        "\n",
        "# Train and evaluate each model\n",
        "results = []\n",
        "for model_name, model in models.items():\n",
        "    if model_name == \"XGBoost\":\n",
        "        model.fit(\n",
        "            X_train, y_train, group=np.bincount(groups_train.astype(int))  # Groups as group sizes\n",
        "        )\n",
        "    elif model_name == \"CatBoost\":\n",
        "        model.fit(\n",
        "            X_train_sorted, y_train_sorted, group_id=groups_train_sorted\n",
        "        )\n",
        "    elif model_name == \"LightGBM\":\n",
        "        model.fit(\n",
        "            X_train, y_train, group=np.bincount(groups_train.astype(int))  # Groups as group sizes\n",
        "        )\n",
        "\n",
        "    # Predict relevance scores\n",
        "    y_pred = model.predict(X_test)\n",
        "\n",
        "    # Evaluate using Normalized Discounted Cumulative Gain (NDCG)\n",
        "    ndcg = ndcg_score([y_test_sorted], [y_pred], k=10)  # NDCG@10\n",
        "    results.append((model_name, ndcg))\n",
        "\n",
        "# Convert results to DataFrame for display\n",
        "results_df = pd.DataFrame(results, columns=[\"Model\", \"NDCG@10\"])\n",
        "\n",
        "# Display results\n",
        "print(results_df)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PA-wtLLwkemo",
        "outputId": "9230c3ea-a6e4-4865-afad-412fc162445e"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000435 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 507\n",
            "[LightGBM] [Info] Number of data points in the train set: 80000, number of used features: 2\n",
            "      Model   NDCG@10\n",
            "0   XGBoost  0.721659\n",
            "1  CatBoost  0.702478\n",
            "2  LightGBM  0.766141\n"
          ]
        }
      ]
    }
  ]
}